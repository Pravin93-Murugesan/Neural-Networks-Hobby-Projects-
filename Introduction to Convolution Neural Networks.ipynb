{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Introduction to Convolution Neural Networks (CNN)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### (Have a look at Introduction to Neural Networks notebook before reading this)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now move from fully-connected neural networks to CNN. A fully-connected neural network usually does not work well with image classification because of the huge number of parameters involved. Such a number of parameters could lead to overfitting and poor classification on test data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "CNN is similar to regular neural networks with weights, biases, loss function and, an optimizer. Additionally, there are also convolutional layers, pooling layers, and flatten layers."
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Import necessary libraries"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom keras.models import Sequential\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\n\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Download and load the MNIST dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "(X_train, y_train), (X_test, y_test) = mnist.load_data()",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Data Pre-processing"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "num_classes = 10\n\n# Similar to what was done with the fully-connected neural networks \n# Additionally, the data is converted to 4-D numpy arrays to be able to use in Keras API\nX_train = X_train.reshape(60000,28,28,1)\nX_test = X_test.reshape(10000,28,28,1)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255.0\nX_test /= 255.0\ny_train = to_categorical(y_train,num_classes)\ny_test = to_categorical(y_test, num_classes)",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Check the data dimensions\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(60000, 28, 28, 1)\n(60000, 10)\n(10000, 28, 28, 1)\n(10000, 10)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Convolutional and Pooling Layers"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In images, the knowledge of spatial structure is important. There is little correlation between two individual pixels unless they are close to each other. This leads to the concepts of Convolutional Layers and Pooling Layers."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Convolutional layer is where we extract features from the images. Since, pixels are only related with the adjacent pixels, convolution preserves the relationship between different parts of an image. Convolution is basically filtering the image to decrease the size of the image (decrease in complexity) without loosing the relationship between pixels. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Pooling layers after each convolution layer further reduce the spatial size and the parameters count. This reduction in computational complexity also helps with the overfitting problem. Typically, a pooling size is chosen and we select either the maximum, average, or sum of values inside these pixels."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Since the complexity is vastly reduced due to convolution and pooling layers, a fully connected neural network is built at the end to classify images. "
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Create, compile and train the model\nAn overview of the complete convolutional neural network  \n![CNN model](images/cnn-model.jpg)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Zero padding is a useful technique to preserve spatial structure of input and output (same width and height).\n\nDropout layers reduce overfitting by disregarding some of the neurons while training.\n\nFlatten layers flatten 2-D arrays to 1-D array before building the fully-connected layers.\n\nOne can always experiment with kernel size, pool size, activation functions, dropout rate, and number of neurons in each dense layer to get a better results."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cnn = Sequential()\n\ncnn.add(Conv2D(32, kernel_size=(5,5),input_shape=(28,28,1), padding='same', activation='relu'))\ncnn.add(MaxPooling2D())\ncnn.add(Conv2D(64, kernel_size=(5,5),padding='same', activation='relu'))\ncnn.add(MaxPooling2D())\ncnn.add(Flatten())\ncnn.add(Dense(1024,activation='relu'))\ncnn.add(Dropout(0.2))\ncnn.add(Dense(10,activation='softmax'))\n\ncnn.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nprint(cnn.summary())",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_5 (Conv2D)            (None, 28, 28, 32)        832       \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 14, 14, 64)        51264     \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 7, 7, 64)          0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 3136)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1024)              3212288   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                10250     \n=================================================================\nTotal params: 3,274,634\nTrainable params: 3,274,634\nNon-trainable params: 0\n_________________________________________________________________\nNone\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "results_cnn = cnn.fit(X_train, y_train, epochs=1, verbose=1, validation_data=(X_test,y_test))",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 695s 12ms/step - loss: 0.0600 - accuracy: 0.9816 - val_loss: 0.0425 - val_accuracy: 0.9859\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Accuracy of the model"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Training data\nprint(results_cnn.history['accuracy'])\n# Test/Validation data\nprint(results_cnn.history['val_accuracy'])",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0.98158336]\n[0.9858999848365784]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Individual predictions with the model"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_index = 569\nplt.imshow(X_test[image_index].reshape(28, 28),cmap='Greys')\n\npred = cnn.predict(X_test[image_index].reshape(1, 28, 28, 1))\nprint(pred.argmax())",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "3\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAADepJREFUeJzt3X+MVPW5x/HPc5VGszQqYRQUdGtjzEWTUjNiidcfN42V3lSxf7SBPxBMZZtYE0iaKJIY+Kdorhd7G3PTZHuLUEPpRYpXEsm9mKWJktTGEbWCXMXI2u5lgSE0CImhgk//2EOz4s53hpkzc2b3eb+Szcyc55w9T072s2dmvmfma+4uAPH8Q9ENACgG4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/ENSFndzZ1KlTvbe3t5O7BEIZHBzU0aNHrZF1Wwq/mc2T9DNJF0j6T3d/MrV+b2+vKpVKK7sEkFAulxtet+mn/WZ2gaT/kPRtSbMkLTSzWc3+PgCd1cpr/jmSPnD3D939r5J+I2l+Pm0BaLdWwn+VpD+PejyULfscM+szs4qZVarVagu7A5CnVsI/1psKX/h8sLv3u3vZ3culUqmF3QHIUyvhH5I0c9TjGZIOttYOgE5pJfyvS7rOzL5iZl+StEDStnzaAtBuTQ/1uftpM3tY0v9qZKhvnbvvza0zAG3V0ji/u2+XtD2nXgB0EJf3AkERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFRLs/Sa2aCkE5LOSDrt7uU8mhpv9uzZk6xXKpVkfefOncn6c889l6ybWc3a+vXrk9vef//9yTomrpbCn/lndz+aw+8B0EE87QeCajX8LmmHmb1hZn15NASgM1p92n+rux80s8slvWxm/+fur4xeIfun0CdJV199dYu7A5CXls787n4wuz0i6QVJc8ZYp9/dy+5eLpVKrewOQI6aDr+Z9ZjZl8/el/QtSem3vQF0jVae9l8h6YVsmOlCSb929//JpSsAbdd0+N39Q0lfy7GXQp08eTJZX7p0ac3ali1bktueOXMmWXf3ZD01jl/PihUrkvXh4eFkfdGiRcn6lVdeed49oTsw1AcERfiBoAg/EBThB4Ii/EBQhB8IKo9P9U0It912W7L+9ttvt23fDzzwQLL+6aefJusbN26sWTt06FBy28ceeyxZHxoaStafeeaZZB3dizM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOH/miSeeSNar1WrN2pw5X/gCo8+p9/VlF198cbJe7yO//f39NWsPPvhgcttNmzYl69OmTUvWMX5x5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnz8ybN6/oFmqq99XdqesE3nvvvZb2XS6HnHU9BM78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBU3XF+M1sn6TuSjrj7jdmyKZL+S1KvpEFJ33f3v7SvzdiOHTuWrN988801awcOHEhue8899yTrd999d7KO8auRM/96SedeAbNC0oC7XydpIHsMYBypG353f0XSuaee+ZI2ZPc3SLov574AtFmzr/mvcPdhScpuL8+vJQCd0PY3/Mysz8wqZlZJfQ8egM5qNvyHzWy6JGW3R2qt6O797l5293KpVGpydwDy1mz4t0lanN1fLOnFfNoB0Cl1w29mmyT9XtL1ZjZkZj+Q9KSku8xsv6S7sscAxpG64/zuvrBG6Zs59xLWwMBAsr5o0aJk/dChQzVrkydPTm67Zs2aZB0TF1f4AUERfiAowg8ERfiBoAg/EBThB4Liq7s74Pjx48l6X19fsp4ayqvn9ttvT9YvueSSpn83xjfO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8HfDaa68l6/W+XrsV27dvT9avv/76ZH3BggXJ+lNPPZWsT5kyJVlHcTjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPN3wLXXXpus79+/P1l/9913k/WNGzfWrG3evDm57SeffJKsP/vss8n6zp07k/UbbrihZm3u3LnJbZcvX56s9/T0JOtI48wPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0GZu6dXMFsn6TuSjrj7jdmy1ZKWSqpmq6109/QHxyWVy2WvVCotNYzzs3fv3mT90UcfTdZ3796drLcyp0CrduzYkazfcccdNWuTJk3Ku52uUC6XValUrJF1Gznzr5c0b4zlP3X32dlP3eAD6C51w+/ur0g61oFeAHRQK6/5HzazP5rZOjO7LLeOAHREs+H/uaSvSpotaVjS2lormlmfmVXMrFKtVmutBqDDmgq/ux929zPu/pmkX0iak1i3393L7l4ulUrN9gkgZ02F38ymj3r4XUl78mkHQKfU/UivmW2SdKekqWY2JGmVpDvNbLYklzQo6Ydt7BFAG9Qd588T4/zjz8GDB5P1etcJbNmypWbt1KlTTfXUqCVLltSsrVmzJrnttGnTcu6mM/Ie5wcwARF+ICjCDwRF+IGgCD8QFOEHgmKoD2310Ucf1azVm5p81apVyfqrr77aVE+SNGPGjGT9+eefT9ZvueWWpvfdTgz1AaiL8ANBEX4gKMIPBEX4gaAIPxAU4QeCYoputNU111zTVE2SBgYGkvWnn346WU9dJzA0NJTc9v3330/Wu3Wc/3xw5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnR9e68ML0n+cjjzySrM+aNatm7d57722qp4mEMz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFU3/GY208x+Z2b7zGyvmS3Llk8xs5fNbH92e1n72wU6w92TPxNBI2f+05J+7O7/KOkbkn5kZrMkrZA04O7XSRrIHgMYJ+qG392H3X13dv+EpH2SrpI0X9KGbLUNku5rV5MA8nder/nNrFfS1yX9QdIV7j4sjfyDkHR53s0BaJ+Gw29mkyX9VtJyd//4PLbrM7OKmVWq1WozPQJog4bCb2aTNBL8je6+NVt82MymZ/Xpko6Mta2797t72d3LpVIpj54B5KCRd/tN0i8l7XP30V+Xuk3S4uz+Ykkv5t8egHZp5CO9t0paJOkdM3srW7ZS0pOSNpvZDyT9SdL32tMiojp9+nSyvnbt2mR99erVNWuXXnppctubbropWZ8I6obf3XdJqjXf9zfzbQdAp3CFHxAU4QeCIvxAUIQfCIrwA0ERfiAovrobbTU4OFizduDAgeS2qSm2JWnXrl3J+kUXXVSz9uabbya3rTd9+ETAmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKcH0lbt25N1h966KFk/fjx4zVrp06daqqns+bOnZusv/TSSzVr9T7PHwFnfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IinF+JH38cXpmthMnTiTry5Yty7Odz3n88ceT9Z6enrbteyLgzA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQdUd5zezmZJ+JWmapM8k9bv7z8xstaSlkqrZqivdfXu7GkUxlixZ0lId3auRi3xOS/qxu+82sy9LesPMXs5qP3X3f2tfewDapW743X1Y0nB2/4SZ7ZN0VbsbA9Be5/Wa38x6JX1d0h+yRQ+b2R/NbJ2ZXVZjmz4zq5hZpVqtjrUKgAI0HH4zmyzpt5KWu/vHkn4u6auSZmvkmcHasbZz9353L7t7uVQq5dAygDw0FH4zm6SR4G90962S5O6H3f2Mu38m6ReS5rSvTQB5qxt+MzNJv5S0z92fHrV8+qjVvitpT/7tAWiXRt7tv1XSIknvmNlb2bKVkhaa2WxJLmlQ0g/b0iGAtmjk3f5dkmyMEmP6wDjGFX5AUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgzN07tzOzqqSPRi2aKuloxxo4P93aW7f2JdFbs/Ls7Rp3b+j78joa/i/s3Kzi7uXCGkjo1t66tS+J3ppVVG887QeCIvxAUEWHv7/g/ad0a2/d2pdEb80qpLdCX/MDKE7RZ34ABSkk/GY2z8zeM7MPzGxFET3UYmaDZvaOmb1lZpWCe1lnZkfMbM+oZVPM7GUz25/djjlNWkG9rTaz/8+O3Vtm9i8F9TbTzH5nZvvMbK+ZLcuWF3rsEn0Vctw6/rTfzC6Q9L6kuyQNSXpd0kJ3f7ejjdRgZoOSyu5e+Jiwmd0u6aSkX7n7jdmyf5V0zN2fzP5xXubuj3ZJb6slnSx65uZsQpnpo2eWlnSfpCUq8Ngl+vq+CjhuRZz550j6wN0/dPe/SvqNpPkF9NH13P0VScfOWTxf0obs/gaN/PF0XI3euoK7D7v77uz+CUlnZ5Yu9Ngl+ipEEeG/StKfRz0eUndN+e2SdpjZG2bWV3QzY7gimzb97PTplxfcz7nqztzcSefMLN01x66ZGa/zVkT4x5r9p5uGHG5195skfVvSj7Knt2hMQzM3d8oYM0t3hWZnvM5bEeEfkjRz1OMZkg4W0MeY3P1gdntE0gvqvtmHD5+dJDW7PVJwP3/XTTM3jzWztLrg2HXTjNdFhP91SdeZ2VfM7EuSFkjaVkAfX2BmPdkbMTKzHknfUvfNPrxN0uLs/mJJLxbYy+d0y8zNtWaWVsHHrttmvC7kIp9sKOPfJV0gaZ27/6TjTYzBzK7VyNleGpnE9NdF9mZmmyTdqZFPfR2WtErSf0vaLOlqSX+S9D137/gbbzV6u1MjT13/PnPz2dfYHe7tnyS9KukdSZ9li1dq5PV1Yccu0ddCFXDcuMIPCIor/ICgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBPU3b2oWwKgN4tEAAAAASUVORK5CYII=\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Even though this is not a good handwriting of the number 3, the CNN model correctly classified it."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Model accuracy for the validation dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Val_accuracy_cnn = cnn.evaluate(X_test, y_test)\nVal_accuracy_cnn",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": "10000/10000 [==============================] - 25s 2ms/step\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "[0.042473986373480876, 0.9858999848365784]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### A convolutional neural network has been shown to classify handwritten digits with Tensorflow’s Keras API. It has achieved an accuracy over 98.5% which is better than the accuracy of fully-connected neural networks."
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}